# -*- coding: utf-8 -*-
"""MC-SCL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OVtJPsi9Q97MQYdfa5RS-vds1ag8hyrd

#**Email Classification using from raw mails using Super Classifier Learner Model**
"""

import re
from email import policy ,parser
import email

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import zipfile 
import os

pip install beautifulsoup4

from bs4 import BeautifulSoup

"""**Importing the classification models**"""

from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier

zip=zipfile.ZipFile('archive (2).zip')
zip.extractall()

for dirname, _, filenames in os.walk('/content/ham'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

for dirname, _, filenames in os.walk('/content/spam'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

Non_SM=os.listdir("/content/ham/")
print(f"The Number of Non-Spam Mails are {len(Non_SM)}")
SM=os.listdir("/content/spam/")
print(f"The Number of Spam Mails are {len(SM)}")

percent_spam=len(SM)/(len(SM)+len(Non_SM))
percent_spam
percent_Non_Spam=1-percent_spam
plt.pie([percent_spam,percent_Non_Spam],labels =["Spam Mails","Ham Mails"])
plt.show()

with open(f"../content/ham/{Non_SM[252]}", 'rb') as file:
    email_ham = parser.BytesParser(policy=policy.default).parse(file) 
print(email_ham)

print(f"From :{email_ham.get_all('From')[0]}")

print(f"The contents are :{email_ham.get_content()}")

all_spam_mails=[]
all_ham_mails=[]
for file1 in SM:
    with open(f"../content/spam/{file1}", 'rb') as file:
        all_spam_mails.append(parser.BytesParser(policy=policy.default).parse(file))
for file1 in Non_SM:
    with open(f"../content/ham/{file1}", 'rb') as file:
        all_ham_mails.append(parser.BytesParser(policy=policy.default).parse(file))

len(all_spam_mails)

len(all_ham_mails)

"""**Extracting the content /messagefrom mail** """

def email_content_type(email):
    payload = email.get_payload()
    if isinstance(payload, list):
        return "multipart({})".format(", ".join([email_content_type(sub_email) for sub_email in payload]))
    else:
        return email.get_content_type()

"""**Preview of the above function**"""

email_content_type(all_spam_mails[131])

"""**Removal of HTML tage from the message like sender,reciever etc**"""

def Removal_Html_Tags(mail):
    try:
        soup = BeautifulSoup(mail.get_content(), 'html.parser')
        return (soup.text.replace('\n\n',''))
    except:
        return None

"""**Preview of the above function**"""

print(Removal_Html_Tags(all_spam_mails[129]))

"""**Preprocessing/cleaning the text/content in the mail**"""

def Convertion_Mails(mail):
    for section in mail.walk():
        content_type=section.get_content_type()
        
        if(content_type in ['text/plain','text/html']):
            try: 
                content = section.get_content()
            except:
                content = str(section.get_payload())
            if(content_type=='text/plain'):
                return(content)
            else:
                return (Removal_Html_Tags(section))
        else:
            continue

"""**Preview of the above function**"""

converted_Email=Convertion_Mails(all_spam_mails[129])
print(converted_Email)

"""**Create a dataframe with the label ,mail content for each category**"""

def Create_Dataframe(emails,spam):
    converted_mails=[]
    for mail in range(len(emails)):
        converted_mails.append(Convertion_Mails(emails[mail]))
    dataframe=pd.DataFrame(converted_mails,columns=['Mail'])
    dataframe['Spam or Not']=spam
    return (dataframe)

ham_df=Create_Dataframe(all_ham_mails,0)
spam_df=Create_Dataframe(all_spam_mails,1)

ham_df

"""**Concatenating both the dataframes into a single dataframe**"""

df=pd.concat([ham_df,spam_df],axis=0)
print(len(df))

df = df.dropna()
df = df.sample(frac=1).reset_index(drop=True)
print(len(df))

df

"""**Removal of Escape Sequences in the conetent of mail if any**"""

for i in range(len(df)):
    df['Mail'][i] = re.sub(r"[^a-zA-Z0-9]+", ' ', df['Mail'][i])
(df)

X=df.iloc[:,:-1].values
y=df.iloc[:,-1].values

X

y

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 1500)
X = cv.fit_transform(X.ravel()).toarray()

X

def get_models():
  models=list()
  models.append(LogisticRegression(solver='liblinear'))
  models.append(DecisionTreeClassifier())
  models.append(SVC(gamma='scale', probability=True))
  models.append(GaussianNB())
  models.append(MultinomialNB())
  models.append(KNeighborsClassifier())
  models.append(AdaBoostClassifier())
  models.append(BaggingClassifier(n_estimators=10))
  models.append(RandomForestClassifier(n_estimators=10))
  models.append(ExtraTreesClassifier(n_estimators=10))
  models.append(MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, learning_rate='adaptive', max_iter=500))
  return models

def get_out_of_fold_predictions(X, y, models):
	meta_X, meta_y = list(), list()
	# define split of data
	kfold = KFold(n_splits=10, shuffle=True)
	# enumerate splits
	for train_ix, test_ix in kfold.split(X):
		fold_yhats = list()
		# get data
		train_X, test_X = X[train_ix], X[test_ix]
		train_y, test_y = y[train_ix], y[test_ix]
		meta_y.extend(test_y)
		# fit and make predictions with each sub-model
		for model in models:
			model.fit(train_X, train_y)
			yhat = model.predict_proba(test_X)
			# store columns
			fold_yhats.append(yhat)
		# store fold yhats as columns
		meta_X.append(np.hstack(fold_yhats))
	return np.vstack(meta_X), np.asarray(meta_y)

def base_models(X_train,y_train,models):
  print("Models initialised")
  for i in range(len(models)):
    models[i].fit(X_train,y_train)
    print("{} model training completed".format(str(models[i])))
  print("models construction completed")

def fit_meta_model(X_train, y_train):
	model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, learning_rate='adaptive', max_iter=500)
	model.fit(X_train, y_train)
	return model

def evaluate_models(X_test, y_test, models):
	for model in models:
		yhat = model.predict(X_test)
		acc = accuracy_score(y_test, yhat)
		print('%s: %.3f' % (model.__class__.__name__, acc*100))

def super_learner_predictions(X, models, meta_model):
	meta_X = list()
	for model in models:
		yhat = model.predict_proba(X)
		meta_X.append(yhat)
	meta_X = np.hstack(meta_X)
	# predict
	return meta_model.predict(meta_X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42)

print('Train', X_train.shape, y_train.shape, 'Test', X_test.shape, y_test.shape)

models = get_models()

meta_X, meta_y = get_out_of_fold_predictions(X, y, models)

print('Meta ', meta_X.shape, meta_y.shape)

base_models(X_train, y_train, models)

meta_model = fit_meta_model(meta_X, meta_y)

evaluate_models(X_test, y_test, models)

yhat = super_learner_predictions(X_test, models, meta_model)
print('Super Learner: %.3f' % (accuracy_score(y_test, yhat) * 100))